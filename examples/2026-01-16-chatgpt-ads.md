---
title: "ChatGPT, 무료 티어에 광고 테스트 시작 + 미국에 새로운 $8/월 Go 플랜 출시"
summary:
  - "OpenAI가 ChatGPT Go 플랜($8/월)을 출시하고 무료 및 Go 티어에서 광고 테스트를 시작했다"
  - "중국 연구진은 2026년 AI 병목이 훈련(training)이 아닌 추론(inference)에 있다고 분석했다"
  - "Claude Opus 4.5가 SWE-bench에서 63.3% 해결률로 1위를 기록했다"
  - "FLUX.2 [klein]이 4B 파라미터의 소비자 친화적 모델로 vLLM 지원과 함께 출시됐다"
  - "Cursor Ultra 사용자들이 5분에 $2, 단일 오케스트레이터 실행에 월 할당량 20%를 소비했다고 보고했다"
date: 2026-01-16
originalUrl: "https://news.smol.ai/issues/26-01-16-chatgpt-ads/"
hasHeadline: true
headline: "ChatGPT, 무료 티어에 광고 테스트 시작 + 미국에 새로운 $8/월 Go 플랜 출시"
tags:
  - OpenAI
  - ChatGPT
  - 광고
  - 추론
  - Claude
isFeatured: true
---

## 헤드라인: ChatGPT, 프리미엄 + 광고 모델로 전환

OpenAI가 두 가지 주요 수익화 전략을 발표했다. 새로운 ChatGPT Go 플랜($8/월)은 "10배 더 많은 메시지", 파일 업로드, 이미지 생성, 확장된 메모리, 그리고 무제한 GPT 5.2 인스턴트 접근을 제공한다. 동시에 OpenAI는 무료 및 Go 티어에서 광고 테스트를 시작하며, "광고는 절대로 응답에 영향을 주지 않으며 명확하게 표시된다"고 강조했다.

가격 복잡성에 대해 회의적인 반응이 있었으며, 한 댓글러는 점점 늘어나는 티어 혼란을 지적했다. Sam Altman은 메모리 시스템 개선과 함께 "매우 빠른 Codex 출시 예정"을 반복적으로 예고했다.

---

## AI Twitter Recap

### 수익화 & 제품 변화

OpenAI의 이중 발표는 엇갈린 반응을 불러일으켰다. 일부는 불가피한 광고 확산에 체념했고, 다른 이들은 재정적 인센티브가 시간이 지남에 따라 모델 응답에 미묘하게 편향을 줄 수 있는지 의문을 제기했다.

### 에이전트 워크플로우 & 인프라

엔지니어들은 휴먼-인-더-루프(human-in-the-loop) 시스템이 신뢰성 승수 역할을 한다고 논의했다. 수동 감독을 추가하면 동일한 모델도 훨씬 더 신뢰할 수 있게 느껴진다는 것이다. Jerry Liu는 "정적 청킹(static chunking)은 죽었다"고 주장하며, 동적 확장을 갖춘 파일 기반 검색이 전통적인 RAG 파이프라인을 능가한다고 말했다.

오케스트레이션 도구들이 확산되었다: Anthropic의 Cowork이 주류로 진입했고, sled와 OpenWork 같은 소규모 제품들은 완전 로컬 컴퓨터 에이전트를 위한 Ollama 통합을 추가했다.

### 하드웨어 & 추론(Inference)

중국 연구 요약은 추론(훈련이 아닌)이 2026년의 병목이라고 위치시켰다. Prefill 우세와 컨텍스트 캐싱이 표준이 되고 있으며, Prefill/Decode 분리는 신중한 스케줄링 재설계 없이는 활용도를 위협한다.

Artificial Analysis는 SambaNova 하드웨어에서 DeepSeek R1을 벤치마크하여 NVIDIA 독점 서사에 도전하는 경쟁력 있는 처리량 지표를 보여주었다.

### 모델 & 검색

- OpenBMB가 RTX 4090에서 0.15× 실시간으로 토큰화 없는 음성 복제를 위한 VoxCPM을 오픈소스화했다
- TII가 100M 파라미터 미만 엣지 추론 작업을 위한 Falcon-H1-Tiny를 출시했다
- 멀티-벡터 검색이 훨씬 큰 기준 모델과 경쟁하는 소형 모델의 스케일링 솔루션으로 주목받았다

### 생성형 미디어

FLUX.2 [klein]이 첫날부터 vLLM을 지원하며 4B 파라미터 13GB 미만 소비자 모델로 출시되었다. Kling의 모션 제어 워크플로우는 빠른 캐릭터 교체와 전환 가능한 퍼포먼스 캡처를 가능하게 했다.

---

## AI Reddit Recap

### r/LocalLLama & r/localLLM

2025년 12월 SWE-bench 리더보드는 Claude Opus 4.5가 63.3% 해결률로 선두를 차지했으며, GPT-5.2 xhigh가 61.5%를 기록했다. Gemini 3 Flash Preview는 더 작은 크기에도 불구하고 Pro 변형보다 뛰어난 성능을 보였다. GLM-4.7은 최고의 오픈소스 경쟁자로 순위에 올랐다.

하드웨어 토론은 A100 GPU 업그레이드, M-시리즈 Mac 비교, RTX 5070 Ti 생산 중단이 공급에 미치는 영향에 집중되었다. 사용자들은 llama.cpp와 vLLM 최적화, MoE 아키텍처를 활용하여 10년 된 시스템에서 120B 파라미터 모델을 실행했다고 보고했다.

r/LocalLLM 30일 혁신 콘테스트는 추론/파인튜닝 프로젝트에 RTX PRO 6000과 클라우드 시간을 상품으로 제공했다.

### 비기술 서브레딧

Claude Cowork이 Pro 구독자에게 제공되었지만 사용자들은 파일 정렬 중 사용량 제한에 빠르게 도달했다. Claude Flow v3는 75-80% 토큰 감소와 250% 구독 용량 증가를 약속했지만, 댓글러들은 버즈워드 가득한 마케팅을 사용한 입증되지 않은 성능 주장에 의문을 제기했다.

Gemini 3 Pro는 컨텍스트 윈도우 감소와 함께 성능 저하가 보고되어 사용자들을 GPT 5.2 Thinking과 Opus 4.5 대안으로 이동시켰다.

---

## AI Discord Recap

### 청구 공포 스토리

Cursor Ultra 사용자들은 약 5분에 $2를 소비하고 단일 오케스트레이터 실행에 월 할당량의 20%를 소비했다고 보고했다. 한 Qoder 사용자는 월 $400 지출을 "도박이나 헤로인"에 비유했다. Gemini CLI 워크플로우는 하루 1000만 토큰(약 $120/일 예상)에 도달하여 토큰 집약적 워크플로우의 "조용한 소비자" 문제를 보여주었다.

### 모델 & 도구 업데이트

- Translate Gemma가 다국어 지원과 함께 Hugging Face에 출시됨
- K2 Turbo가 약 73 tps에 도달, 표준 K2보다 약 2.6배 빠름
- Claude가 단일 API 요청에서 병렬 도구 사용을 시연하여 오케스트레이션 오버헤드 감소
- Hawk Ultra가 단일 프롬프트에서 9,500줄 이상의 코드 출력을 주장

### 시스템 현실

Runpod GPU 언더볼팅이 조용한 성능 불일치를 야기했다. PCIe Gen3×1 슬롯은 RTX 3090 처리량을 약 25% 감소시켰다. GPU MODE 엔지니어들은 RDNA3 하드웨어에서 buffer_inv sc1 명령을 사용하여 L2 캐시 일관성을 디버깅했다—"일관성이 없으면 모든 것이 빠르다."

### 탈옥(Jailbreaking) 커뮤니티

BASI Discord 멤버들은 Gemini 탈옥이 빠르게 패치되지만 제한 없는 NSFW 콘텐츠에 가장 쉽다고 언급했다. Sonnet 4.5는 멀티턴 다이어그램 내러티브를 통해 잠금 해제되었고; Llama 3 프롬프트 역전은 거부를 재구성하여 준수를 강제했다.

---

**핵심 요약:** 2026년 인프라 논의는 기업들이 무료 티어를 수익화하는 동안 엔지니어들이 하드웨어 현실과 청구 불확실성을 최적화하면서 추론 최적화, 소형 모델 스케일링, 비용 가시성을 강조한다.
